{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0d0b86-ebf5-4a65-aaae-513493ad9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42d86dc-6b18-418e-9942-c3b15dddb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f41619-8726-4e90-b2c8-0ae295009e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"train_finetune.jsonl\")[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04dceaca-3c84-4099-a112-a7f66811e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": f\"### Instruction:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db283fd8-4649-4296-9784-c4c1d9e77313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your Hugging Face access token here\n",
    "login(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90979760-3eaa-4d28-9072-28024095bee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cc1253670c4558baccd58080b70df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Important: LLaMA 2 tokenizer may not have a pad token\n",
    "# safest is to set pad_token = eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # will put model on GPU(s) if available\n",
    "    dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108bd7f3-7ae2-4326-b79b-2d1da25a25f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Context:\n",
      "Why does a tree make sound when it crashes to the ground? How does the sound reach peoples ears if they happen to be in the forest? And in general, how do sounds get started, and how do they travel? Keep reading to find out. All sounds begin with vibrating matter. It could be the ground vibrating when a tree comes crashing down. Or it could be guitar strings vibrating when they are plucked. You can see a guitar string vibrating in Figure 20.2. The vibrating string repeatedly pushes against the air particles next to it. The pressure of the vibrating string causes these air particles to vibrate. The air particles alternately push together and spread apart. This starts waves of vibrations that travel through the air in all directions away from the strings. The vibrations pass through the air as longitudinal waves, with individual air particles vibrating back and forth in the same direction that the waves travel. You can see an animation of sound waves moving through air at this URL: Sound waves are mechanical waves, so they can travel only though matter and not through empty space. This was demonstrated in the 1600s by a scientist named Robert Boyle. Boyle placed a ticking clock in a sealed glass jar. The clock could be heard ticking through the air and glass of the jar. Then Boyle pumped the air out of the jar. The clock was still running, but the ticking could no longer be heard. Thats because the sound couldnt travel away from the clock without air particles to pass the sound energy along. You can see an online demonstration of the same experimentwith a modern twistat this URL:  (4:06). MEDIA Click image to the left or use the URL below. URL:  Sound waves can travel through many different kinds of matter. Most of the sounds we hear travel through air, but sounds can also travel through liquids such as water and solids such as glass and metal. If you swim underwater or even submerge your ears in bathwater any sounds you hear have traveled to your ears through water. You can tell that sounds travel through glass and other solids because you can hear loud outdoor sounds such as sirens through closed windows and doors. Sound has certain characteristic properties because of the way sound energy travels in waves. Properties of sound include speed, loudness, and pitch. The speed of sound is the distance that sound waves travel in a given amount of time. You probably already know that sound travels more slowly than light. Thats why you usually see the flash of lightning before you hear the boom of thunder. However, the speed of sound isnt constant. It varies depending on the medium of the sound waves. Table 20.1 lists the speed of sound in several different media. Generally, sound waves travel fastest through solids and slowest through gases. Thats because the particles of solids are close together and can quickly pass the energy of vibrations to nearby particles. You can explore the speed of sound in different media at this URL:  Medium (20C) Air Water Wood Glass Aluminum Speed of Sound Waves (m/s) 343 1437 3850 4540 6320 The speed of sound also depends on the temperature of the medium. For a given medium such as air, sound has a slower speed at lower temperatures. You can compare the speed of sound in air at different temperatures in Table transfer the energy of the sound waves. The amount of water vapor in the air affects the speed of sound as well. Do you think sound travels faster or slower when the air contains more water vapor? (Hint: Compare the speed of sound in water and air in Table 20.1.) Temperature of Air 0C 20C 100C Speed of Sound (m/s) 331 343 386 KQED: Speed of Sound Along with cable cars and seagulls, the Golden Gate Bridge foghorn is one of San Franciscos most iconic sounds. But did you know that if you hear that foghorn off in the distance, you can calculate how many miles you are from the bridge? Using the Speed of Sound exhibit at the Outdoor Exploratorium at Fort Mason, Shawn Lani shows us how sound perception is affected by distance. For more information on the speed of sound, see http://science.kqed. MEDIA Click image to the left or use the URL below. URL: A friend whispers to you in class in a voice so soft that you have to lean very close to hear what hes saying. Later that day, your friend shouts to you across the football field. Now his voice is loud enough for you to hear him clearly even though hes many meters away. Obviously, sounds can vary in loudness. Loudness refers to how loud or soft a sound seems to a listener. The loudness of sound is determined, in turn, by the intensity of sound. Intensity is a measure of the amount of energy in sound waves. The unit of intensity is the decibel (dB). You can see typical decibel levels of several different sounds in Figure 20.3. As decibel levels get higher, sound waves have greater intensity and sounds are louder. For every 10-decibel increase in the intensity of sound, loudness is 10 times greater. Therefore, a 30-decibel \"quiet\" room is 10 times louder than a 20-decibel whisper, and a 40- decibel light rainfall is 100 times louder than a 20-decibel whisper. How much louder than a 20-decibel whisper is the 60-decibel sound of a vacuum cleaner? The intensity of sound waves determines the loudness of sounds, but what determines intensity? Intensity is a function of two factors: the amplitude of the sound waves and how far they have traveled from the source of the sound. Remember that sound waves start at a source of vibrations and spread out from the source in all directions. The farther the sound waves travel away from the source, the more spread out their energy becomes. This is illustrated in Figure 20.4. The decrease in intensity with distance from a sound source explains why even loud sounds fade away as you move farther from the source. It also explains why low-amplitude sounds can be heard only over short distances. For a video demonstration of the amplitude and loudness of sounds, go to this URL: interactive animation at this URL: A marching band is parading down the street. You can hear it coming from several blocks away. When the different instruments finally pass by you, their distinctive sounds can be heard. The tiny piccolos trill their bird-like high notes, and the big tubas rumble out their booming bass notes (see Figure 20.5). Clearly, some sounds are higher or lower than others. But do you know why? How high or low a sound seems to a listener is its pitch. Pitch, in turn, depends on the frequency of sound waves. Recall that the frequency of waves is the number of waves that pass a fixed point in a given amount of time. High-pitched sounds, like the sounds of a piccolo, have high-frequency waves. Low-pitched sounds, like the sounds of a tuba, have low-frequency waves. For a video demonstration of frequency and pitch, go to this URL:  (3:20). MEDIA Click image to the left or use the URL below. URL:  To explore an interactive animation of sound wave frequency, go to this URL:  The frequency of sound waves is measured in hertz (Hz), or the number of waves that pass a fixed point in a second. Human beings can normally hear sounds with a frequency between about 20 Hz and 20,000 Hz. Sounds with frequencies below 20 hertz are called infrasound. Sounds with frequencies above 20,000 hertz are called ultrasound. Some other animals can hear sounds in the ultrasound range. For example, dogs can hear sounds with frequencies as high as 50,000 Hz. You may have seen special whistles that dogs but not people can hear. The whistles produce a sound with a frequency too high for the human ear to detect. Other animals can hear even higher-frequency sounds. Bats, for example, can hear sounds with frequencies higher than 100,000 Hz. Look at the police car in Figure 20.6. The sound waves from its siren travel outward in all directions. Because the car is racing forward (toward the right), the sound waves get bunched up in front of the car and spread out behind it. As the car approaches the person on the right (position B), the sound waves get closer and closer together. In other words, they have a higher frequency. This makes the siren sound higher in pitch. After the car speeds by the person on the left (position A), the sound waves get more and more spread out, so they have a lower frequency. This makes the siren sound lower in pitch. A change in the frequency of sound waves, relative to a stationary listener, when the source of the sound waves is moving is called the Doppler effect. Youve probably experienced the Doppler effect yourself. The next time a vehicle with a siren races by, listen for the change in pitch. For an online animation of the Doppler effect, go to the URL below. The tree in Figure 20.1 fell to the forest floor in a high wind. Does it make you think of an old riddle? The riddle goes like this: If a tree falls in the forest and theres no one there to hear it, does it make any sound? To answer the riddle correctly, you first need to know the scientific definition of sound. In science, sound is the transfer of energy from a vibrating object in waves that travel through matter. Most people commonly use the term sound to mean what they hear when sound waves enter their ears. The tree creates sound waves when it falls to the ground, so it makes sound according to the scientific definition. But the sound wont be detected by a persons ears if theres no one in the forest. So the answer to the riddle is both yes and no! Sound is the transfer of energy from a vibrating object in waves that travel through matter. Properties of sound include speed, loudness, and pitch. The speed of sound varies in different media. The loudness of sound depends on the intensity of sound waves. The pitch of sound depends on the frequency of sound waves. Doppler effect: Change in the frequency and pitch of sound that occurs when the source of the sound is moving relative to the listener. infrasound: Sound with a frequency below the range of human hearing (less than 20 hertz). intensity: Measure of the amount of energy in sound waves, which is determined by the amplitude of the waves and how far they have traveled and spread out from the source of the sound. loudness: How a listener perceives the intensity of sound. pitch: How high or low a sound seems to a listener. sound: Transfer of energy from a vibrating object in longitudinal waves that travel through matter. ultrasound: Sound with a frequency above the range of human hearing (greater than 20,000 hertz).\n",
      "\n",
      "Question:\n",
      "The intensity of sound waves is the same regardless of distance from the sound source.\n",
      "a) true\n",
      "b) false\n",
      "\n",
      "### Response:\n",
      "Answer: b) false\n"
     ]
    }
   ],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = split_dataset[\"train\"]\n",
    "val_data = split_dataset[\"test\"]\n",
    "\n",
    "print(train_data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6eef1c-8a10-4715-b31b-950de66c71fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1341e7f57a7447bacf1277e348cd597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/866 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_tokenized = train_data.map(tokenize, batched=True, remove_columns=train_data.column_names)\n",
    "val_tokenized = val_data.map(tokenize, batched=True, remove_columns=val_data.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e02098-2f49-4909-aad0-df22c4569840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9217ece3b40845a9a87cd0fb3b354330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e7b8d8-1d55-468e-abfc-908987230592",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b21ff8-de1d-4882-8486-ca8bdb225d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e1eb42-c7e5-462e-87cd-f398d0408e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/tinyllama-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",   # still valid\n",
    "    eval_strategy=\"epoch\",   # <-- fix for older versions\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "900cd647-7aec-4589-8b8d-3903a7abe42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbec60b-c6b6-4a4a-bc12-e4f432266715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2922' max='2922' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2922/2922 56:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>0.405009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.184900</td>\n",
       "      <td>0.174156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.101879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2922, training_loss=0.3819257597006159, metrics={'train_runtime': 3390.6057, 'train_samples_per_second': 6.89, 'train_steps_per_second': 0.862, 'total_flos': 4.747779701295022e+17, 'train_loss': 0.3819257597006159, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5a92d3d-1df6-4158-9352-8f5cb743b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"new_fine_tunned_lama2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bdbd2a5-9761-4afc-bdb2-fcab8384298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Reload tokenizer (fix padding issue for LLaMA)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Define preprocess function\n",
    "def preprocess(examples):\n",
    "    combined = [f\"{i}\\n{o}\" for i, o in zip(examples[\"input\"], examples[\"output\"])]\n",
    "    return tokenizer(\n",
    "        combined,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # or \"longest\" if you prefer dynamic padding\n",
    "        max_length=1024\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3972b0d2-4f68-403d-9bed-b23dd51274a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6891453cc1e34d62a2045f3ee765e2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files={\"test\": \"test_finetune.jsonl\"})\n",
    "\n",
    "# 4. Preprocess test dataset\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset[\"test\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a7f85ac-42b8-4ac6-a4ad-b1cfedf7efb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [314/314 03:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: {'eval_loss': 2.900541067123413, 'eval_runtime': 226.7565, 'eval_samples_per_second': 11.078, 'eval_steps_per_second': 1.385, 'epoch': 3.0}\n",
      "Perplexity: 18.18398146275199\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate with Trainer (loss + perplexity)\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_test[\"test\"])\n",
    "print(\"Test Metrics:\", metrics)\n",
    "print(\"Perplexity:\", math.exp(metrics[\"eval_loss\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71827e35-6673-4108-83e8-f8c74cc8f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|‚ñç         | 95/2512 [05:25<1:54:33,  2.84s/it]"
     ]
    }
   ],
   "source": [
    "# 6. Exact-Match Accuracy Evaluation\n",
    "predictions_data = []\n",
    "correct, total = 0, 0\n",
    "\n",
    "for example in tqdm(test_dataset[\"test\"], desc=\"Evaluating\"):\n",
    "    inputs = tokenizer(example[\"input\"], return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    match = example[\"output\"].strip() in prediction\n",
    "    if match:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "    predictions_data.append({\n",
    "        \"input\": example[\"input\"],\n",
    "        \"expected_output\": example[\"output\"],\n",
    "        \"model_prediction\": prediction,\n",
    "        \"match\": match\n",
    "    })\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\nExact-Match Accuracy on Test Set: {accuracy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f415432-13a7-4351-ae92-fa39d119188b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
